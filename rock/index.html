<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Neural Radiance Flow for 4D View Synthesis and Video Processing">
    <meta name="author"
          content="Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B. Tenenbaum, Jiajun Wu">

    <title>Neural Radiance Flow for 4D View Synthesis and Video Processing</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div align="center">
        <div class="container"></div>
        <div align="center">
        <h2>Sim2Real Object-Centric Keypoint Detection and Description</h2>
        <h3>AAAI 2022</h3>
    </div>

    <hr>
    <p class="authors">
        <a href="https://yilundu.github.io">Chengliang Zhong<sup>*12</sup></a>,
        <a href="">Chao Yang<sup>*2</sup></a>,
        <a href="https://kovenyu.com/">Jinshan Qi<sup>3</sup></a>,
        <a href="https://mitibmwatsonailab.mit.edu/people/joshua-tenenbaum/">Fuchun Sun<sup>2</sup></a>,
        <a href="https://jiajunwu.com/">Huaping Liu<sup>2</sup></a>
        <a href="https://jiajunwu.com/">Xiaodong Mu<sup>1</sup></a>
        <a href="https://jiajunwu.com/">Wenbing Huangu<sup>2</sup></a>
    </p>
    <p class="institution">
      <sup>1</sup> Xi’an Research Institute of High-Tech, Xi’an 710025, China&nbsp;&nbsp;&nbsp;
      <br>
      <sup>2</sup> Beijing National Research Center for Information Science and Technology (BNRist),State Key Lab on Intelligent Technology and Systems,Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China
      <br>
      <sup>3</sup> Shandong University of Science and Technology, Qingdao 266590, China
    </p>

    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2012.09790">Paper</a>
        <a class="btn btn-primary" href="https://github.com/yilundu/nerflow">Code</a>
    </div>
</div>

<hr>
<div class="container">
    <div class="section">
        <div align="center">
            <h2>Abstract</h2>
        </div>    
        <p style="text-align:justify; text-justify:inter-ideograph;">
                Keypoint detection and description play a central role in
                computer vision. Most existing methods are in the form of
                scene-level prediction, without returning the object classes
                of different keypoints. In this paper, we propose the object-
                centric formulation, which, beyond the conventional setting,
                requires further identifying which object each interest point
                belongs to. With such fine-grained information, our frame-
                work enables more downstream potentials, such as object-
                level matching and pose estimation in a clustered environ-
                ment. To get around the difficulty of label collection in the
                real world, we develop a sim2real contrastive learning mech-
                anism that can generalize the model trained in simulation to
                real-world applications. The novelties of our training method
                are three-fold: (i) we integrate the uncertainty into the learn-
                ing framework to improve feature description of hard cases,
                e.g., less-textured or symmetric patches; (ii) we decouple
                the object descriptor into two output branches—intra-object
                salience and inter-object distinctness, resulting in a better
                pixel-wise description; (iii) we enforce cross-view semantic
                consistency for enhanced robustness in representation learn-
                ing. Comprehensive experiments on image matching and 6D
                pose estimation verify the encouraging generalization ability
                of our method from simulation to reality. Particularly for 6D
                pose estimation, our method significantly outperforms typical
                unsupervised/sim2real methods, achieving a closer gap with
                the fully supervised counterpart.
            </p>


        </br>

        <div class="row justify-content-left">
            <div class="col-sm-6">
                <video src="video/1_cracker_box.mp4" controls="controls" width="540" height="480" >
                </video>
            </div>
            <div class="col-sm-6">
                <video src="video/2_sugar_box.mp4" controls="controls" width="540" height="480">
                </video>
            </div>
        </div>
        
        <div class="row justify-content-left">
            <div class="col-sm-6">
                <video src="video/3_tomato_soup_can.mp4" controls="controls" width="540" height="480" >
                </video>
            </div>
            <div class="col-sm-6">
                <video src="video/4_mustard_bottle.mp4" controls="controls" width="540" height="480">
                </video>
            </div>
         </div>

    </div>


    <div class="section">
        <h2>Sparse View Synthesis</h2>
        <hr>
        <p>
            Our approach is able to represent a dynamic scene when given only a limited number of views across the scene, as well as a sparse set of timestamps from which the underlying scene is captured with. We illustrate the ability to do novel view synthesis utilizing a dynamic scene captured from a single real monocular video. </p>

        <div class="row justify-content-left">
            <div class="col-sm-4">
                <img src="fig/ayush.gif" class="img-fluid" style="width:100%">
            </div>
            <div class="col-sm-2">
                <img src="fig/person_real.gif" class="img-fluid">
            </div>
            <div class="col-sm-6">
                <img src="fig/kid_real.gif" class="img-fluid" style="width:100%">
            </div>
         </div>

        </br>

        <p> 
        Next, we assess the ability of NeRFlow to synthesize scenes which are captured by a sparse set of timestamps. We capture the pouring animation utilizing only 1 in 10 frames of animation.  We animate the resultant pouring animation across frames of animation and find that NeRFlow with consistency (left) performs significantly better than a approach without consistency (right).
        </p>

        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src='fig/joint.gif' class='img-fluid'>
            </div>
        </div>


    </div>

    <div class="section">
        <h2>Video Processing</h2>
        <hr>
        <p>
        NeRFlow is able to capture and aggregate radiance information across different viewpoints and timesteps. By rendering from this aggregate representation, NeRFlow can serve as a scene prior for video processing tasks. We present results where we show that when NeRFlow is fit on noisy input images, renders from NeRFlow are able to denoise and super-resolve the input images.
        </p>

        <div class="row justify-content-left">
            <div class="col-sm-4">
                <img src="fig/denoise.gif" class="img-fluid">
            </div>
            <div class="col-sm-4">
                <img src="fig/superres.gif" class="img-fluid">
            </div>
            <div class="col-sm-3">
                <img src="fig/denoise_real.gif" class="img-fluid" style="width:95%">
            </div>
         </div>



    </div>

            <div class="section">
                <h2>Paper</h2>
                <hr>
                <div>
                    <div class="list-group">
                        <a href="https://arxiv.org/abs/2012.09790"
                           class="list-group-item">
                            <img src="fig/paper_thumbnail.png"
                                 style="width:100%; margin-right:-20px; margin-top:-10px;">
                        </a>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>Bibtex</h2>
                <hr>
                <div class="bibtexsection">
                    @inproceedings{du2021nerflow,
                      author    = {Yilun Du and Yinan Zhang and Hong-Xing Yu 
                                   and Joshua B. Tenenbaum and Jiajun Wu},
                      title     = {Neural Radiance Flow for 4D View Synthesis and Video Processing},
                      year      = {2021},
                      booktitle   = {Proceedings of the IEEE/CVF International Conference
                                     on Computer Vision},
                    }
                </div>
            </div>

            <hr>

            <footer>
                <p>Send feedback and questions to <a href="https://yilundu.github.io">Yilun Du</a></p>
            </footer>
        </div>

</body>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</html>
